import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import onnx
import onnxruntime as ort
from sklearn.preprocessing import StandardScaler
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import time
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class OriginalLSTMHealthMonitor(nn.Module):
    """åŸå§‹LSTMå¥åº·ç›‘æµ‹æ¨¡å‹"""

    def __init__(self, input_size=4, hidden_size=64, num_layers=2, num_classes=2, dropout=0.2):
        super(OriginalLSTMHealthMonitor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )

    def forward(self, x):
        batch_size = x.size(0)

        lstm_out, _ = self.lstm(x)
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        final_out = attn_out[:, -1, :]
        output = self.classifier(final_out)

        return output


class AdvancedONNXCompatibleLSTMHealthMonitor(nn.Module):
    """
    é«˜çº§ONNXå…¼å®¹æ¨¡å‹ - æ›´ç²¾ç¡®çš„æ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£
    """

    def __init__(self, input_size=4, hidden_size=64, num_layers=2, num_classes=2, dropout=0.2):
        super(AdvancedONNXCompatibleLSTMHealthMonitor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.embed_dim = hidden_size * 2

        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # æ¨¡æ‹ŸMultiheadAttentionçš„å¤šä¸ªç»„ä»¶
        self.num_heads = 8
        self.head_dim = self.embed_dim // self.num_heads

        # æŸ¥è¯¢ã€é”®ã€å€¼çš„çº¿æ€§å˜æ¢
        self.query_projection = nn.Linear(self.embed_dim, self.embed_dim)
        self.key_projection = nn.Linear(self.embed_dim, self.embed_dim)
        self.value_projection = nn.Linear(self.embed_dim, self.embed_dim)

        # è¾“å‡ºæŠ•å½±
        self.out_projection = nn.Linear(self.embed_dim, self.embed_dim)

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )

    def forward(self, x):
        batch_size, seq_len, _ = x.size()

        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, embed_dim)

        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        attn_out = self._scaled_dot_product_attention(lstm_out, lstm_out, lstm_out)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        final_out = attn_out[:, -1, :]  # (batch_size, embed_dim)

        # åˆ†ç±»
        output = self.classifier(final_out)

        return output

    def _scaled_dot_product_attention(self, query, key, value):
        """
        ONNXå…¼å®¹çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶
        """
        batch_size, seq_len, embed_dim = query.size()

        # çº¿æ€§å˜æ¢
        Q = self.query_projection(query)  # (batch_size, seq_len, embed_dim)
        K = self.key_projection(key)  # (batch_size, seq_len, embed_dim)
        V = self.value_projection(value)  # (batch_size, seq_len, embed_dim)

        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # å½¢çŠ¶: (batch_size, num_heads, seq_len, head_dim)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        # å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)

        # Softmax
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended_values = torch.matmul(attention_weights, V)
        # å½¢çŠ¶: (batch_size, num_heads, seq_len, head_dim)

        # é‡æ–°ç»„åˆå¤šå¤´
        attended_values = attended_values.transpose(1, 2).contiguous().view(
            batch_size, seq_len, embed_dim)

        # è¾“å‡ºæŠ•å½±
        output = self.out_projection(attended_values)

        return output


class AdvancedModelConverter:
    """
    é«˜çº§æ¨¡å‹è½¬æ¢å™¨ - ç²¾ç¡®æƒé‡æ˜ å°„
    """

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def load_pytorch_model(self, model_path, model_config=None):
        """åŠ è½½PyTorchæ¨¡å‹å¹¶è½¬æ¢ä¸ºé«˜çº§ONNXå…¼å®¹ç‰ˆæœ¬"""
        print(f"ğŸ“‚ åŠ è½½PyTorchæ¨¡å‹: {model_path}")

        if model_config is None:
            model_config = {
                'input_size': 4,
                'hidden_size': 64,
                'num_layers': 2,
                'num_classes': 2,
                'dropout': 0.2
            }

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=self.device)

        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint

        print("ğŸ” åˆ†ææ¨¡å‹ç»“æ„...")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«MultiheadAttention
        has_multihead_attention = any('attention.in_proj_weight' in key for key in state_dict.keys())

        if has_multihead_attention:
            print("   - æ£€æµ‹åˆ°MultiheadAttentionç»“æ„")
            print("ğŸ”„ åŠ è½½åŸå§‹æ¨¡å‹å¹¶è¿›è¡Œé«˜ç²¾åº¦è½¬æ¢...")

            # åˆ›å»ºåŸå§‹æ¨¡å‹å¹¶åŠ è½½æƒé‡
            original_model = OriginalLSTMHealthMonitor(**model_config)
            original_model.load_state_dict(state_dict)
            original_model.eval()

            # è½¬æ¢ä¸ºé«˜çº§ONNXå…¼å®¹æ¨¡å‹
            onnx_model = self._advanced_convert_to_onnx_compatible(original_model, model_config)

        else:
            print("   - æ£€æµ‹åˆ°é«˜çº§æ³¨æ„åŠ›ç»“æ„")
            onnx_model = AdvancedONNXCompatibleLSTMHealthMonitor(**model_config)
            onnx_model.load_state_dict(state_dict)
            onnx_model.eval()

        print(f"âœ… PyTorchæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   - æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in onnx_model.parameters()):,}")

        return onnx_model, model_config

    def _advanced_convert_to_onnx_compatible(self, original_model, config):
        """é«˜ç²¾åº¦æ¨¡å‹è½¬æ¢"""
        print("ğŸ”„ æ‰§è¡Œé«˜ç²¾åº¦æ¨¡å‹è½¬æ¢...")

        # åˆ›å»ºæ–°çš„é«˜çº§ONNXå…¼å®¹æ¨¡å‹
        onnx_model = AdvancedONNXCompatibleLSTMHealthMonitor(**config)

        # 1. å¤åˆ¶LSTMæƒé‡
        print("   - å¤åˆ¶LSTMæƒé‡...")
        onnx_model.lstm.load_state_dict(original_model.lstm.state_dict())

        # 2. å¤åˆ¶åˆ†ç±»å™¨æƒé‡
        print("   - å¤åˆ¶åˆ†ç±»å™¨æƒé‡...")
        onnx_model.classifier.load_state_dict(original_model.classifier.state_dict())

        # 3. ç²¾ç¡®è½¬æ¢æ³¨æ„åŠ›æƒé‡
        print("   - æ‰§è¡Œç²¾ç¡®æ³¨æ„åŠ›æƒé‡è½¬æ¢...")
        self._precise_attention_weight_conversion(original_model.attention, onnx_model, config)

        # 4. éªŒè¯è½¬æ¢ç²¾åº¦
        print("   - éªŒè¯è½¬æ¢ç²¾åº¦...")
        self._verify_advanced_conversion(original_model, onnx_model, config)

        print("âœ… é«˜ç²¾åº¦æ¨¡å‹è½¬æ¢å®Œæˆ")
        return onnx_model

    def _precise_attention_weight_conversion(self, original_attention, onnx_model, config):
        """ç²¾ç¡®çš„æ³¨æ„åŠ›æƒé‡è½¬æ¢"""
        embed_dim = config['hidden_size'] * 2

        with torch.no_grad():
            # è·å–åŸå§‹MultiheadAttentionæƒé‡
            in_proj_weight = original_attention.in_proj_weight  # [3*embed_dim, embed_dim]
            in_proj_bias = original_attention.in_proj_bias  # [3*embed_dim]
            out_proj_weight = original_attention.out_proj.weight  # [embed_dim, embed_dim]
            out_proj_bias = original_attention.out_proj.bias  # [embed_dim]

            # åˆ†è§£æŸ¥è¯¢ã€é”®ã€å€¼æƒé‡
            query_weight = in_proj_weight[:embed_dim, :]  # [embed_dim, embed_dim]
            key_weight = in_proj_weight[embed_dim:2 * embed_dim, :]  # [embed_dim, embed_dim]
            value_weight = in_proj_weight[2 * embed_dim:, :]  # [embed_dim, embed_dim]

            query_bias = in_proj_bias[:embed_dim]  # [embed_dim]
            key_bias = in_proj_bias[embed_dim:2 * embed_dim]  # [embed_dim]
            value_bias = in_proj_bias[2 * embed_dim:]  # [embed_dim]

            # ç²¾ç¡®å¤åˆ¶æƒé‡åˆ°æ–°æ¨¡å‹
            onnx_model.query_projection.weight.data = query_weight
            onnx_model.query_projection.bias.data = query_bias

            onnx_model.key_projection.weight.data = key_weight
            onnx_model.key_projection.bias.data = key_bias

            onnx_model.value_projection.weight.data = value_weight
            onnx_model.value_projection.bias.data = value_bias

            onnx_model.out_projection.weight.data = out_proj_weight
            onnx_model.out_projection.bias.data = out_proj_bias

            print(f"     * æŸ¥è¯¢æƒé‡: {query_weight.shape}")
            print(f"     * é”®æƒé‡: {key_weight.shape}")
            print(f"     * å€¼æƒé‡: {value_weight.shape}")
            print(f"     * è¾“å‡ºæƒé‡: {out_proj_weight.shape}")

    def _verify_advanced_conversion(self, original_model, onnx_model, config):
        """éªŒè¯é«˜ç²¾åº¦è½¬æ¢"""
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ ·æœ¬
        test_inputs = [
            torch.randn(1, 10, config['input_size']).to(self.device),
            torch.randn(2, 10, config['input_size']).to(self.device),
            torch.randn(4, 10, config['input_size']).to(self.device),
        ]

        original_model.to(self.device)
        onnx_model.to(self.device)

        total_diff = 0
        num_tests = 0

        with torch.no_grad():
            for i, test_input in enumerate(test_inputs):
                # åŸå§‹æ¨¡å‹è¾“å‡º
                original_output = original_model(test_input)

                # ONNXå…¼å®¹æ¨¡å‹è¾“å‡º
                onnx_output = onnx_model(test_input)

                # è®¡ç®—è¾“å‡ºå·®å¼‚
                output_diff = torch.abs(original_output - onnx_output).mean().item()
                total_diff += output_diff
                num_tests += 1

                print(f"     * æµ‹è¯• {i + 1} è¾“å‡ºå·®å¼‚: {output_diff:.6f}")

        avg_diff = total_diff / num_tests
        print(f"     * å¹³å‡è¾“å‡ºå·®å¼‚: {avg_diff:.6f}")

        if avg_diff < 0.01:
            print("     * âœ… é«˜ç²¾åº¦è½¬æ¢éªŒè¯é€šè¿‡")
        elif avg_diff < 0.1:
            print("     * âš ï¸  è½¬æ¢ç²¾åº¦å¯æ¥å—")
        else:
            print("     * âŒ è½¬æ¢ç²¾åº¦ä¸è¶³")

    def convert_to_onnx(self, pytorch_model, onnx_path, input_shape=(1, 10, 4),
                        dynamic_axes=None, opset_version=13):
        """è½¬æ¢ä¸ºONNXæ ¼å¼"""
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢æ¨¡å‹ä¸ºONNXæ ¼å¼...")

        dummy_input = torch.randn(input_shape).to(self.device)
        pytorch_model = pytorch_model.to(self.device)

        print(f"   - è¾“å…¥å½¢çŠ¶: {input_shape}")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - ONNXç‰ˆæœ¬: {opset_version}")

        if dynamic_axes is None:
            dynamic_axes = {
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }

        try:
            pytorch_model.eval()

            # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
            with torch.no_grad():
                test_output = pytorch_model(dummy_input)
                print(f"   - æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {test_output.shape}")

            # å¯¼å‡ºONNXæ¨¡å‹
            torch.onnx.export(
                pytorch_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes=dynamic_axes,
                verbose=False
            )

            print(f"âœ… ONNXæ¨¡å‹è½¬æ¢æˆåŠŸ!")
            print(f"   - ä¿å­˜è·¯å¾„: {onnx_path}")

            # éªŒè¯ONNXæ¨¡å‹
            self._verify_onnx_model(onnx_path)

            return True

        except Exception as e:
            print(f"âŒ ONNXè½¬æ¢å¤±è´¥: {str(e)}")

            if opset_version < 15:
                print(f"ğŸ”„ å°è¯•ä½¿ç”¨ONNX opsetç‰ˆæœ¬ {opset_version + 1}...")
                return self.convert_to_onnx(pytorch_model, onnx_path, input_shape,
                                            dynamic_axes, opset_version + 1)

            return False

    def _verify_onnx_model(self, onnx_path):
        """éªŒè¯ONNXæ¨¡å‹"""
        try:
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)

            print(f"âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
            print(f"   - æ¨¡å‹ç‰ˆæœ¬: {onnx_model.ir_version}")
            print(f"   - æ“ä½œé›†ç‰ˆæœ¬: {onnx_model.opset_import[0].version}")

            try:
                sess = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
                input_shape = sess.get_inputs()[0].shape
                print(f"   - è¾“å…¥å½¢çŠ¶: {input_shape}")

                dummy_input = np.random.randn(1, 10, 4).astype(np.float32)
                output = sess.run(None, {sess.get_inputs()[0].name: dummy_input})
                print(f"   - è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
                print(f"âœ… ONNXè¿è¡Œæ—¶æµ‹è¯•é€šè¿‡")

            except Exception as e:
                print(f"âš ï¸  ONNXè¿è¡Œæ—¶æµ‹è¯•å¤±è´¥: {str(e)}")

        except Exception as e:
            print(f"âš ï¸  ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {str(e)}")


class ONNXModelTester:
    """ONNXæ¨¡å‹æµ‹è¯•å™¨ï¼ˆä¿æŒä¸å˜ï¼‰"""

    def __init__(self, onnx_path, scaler_path=None):
        self.onnx_path = onnx_path
        self.scaler_path = scaler_path
        self.scaler = None

        print(f"ğŸ”§ åˆå§‹åŒ–ONNXè¿è¡Œæ—¶...")

        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        providers = ['CPUExecutionProvider']
        if torch.cuda.is_available():
            providers.insert(0, 'CUDAExecutionProvider')

        self.ort_session = ort.InferenceSession(onnx_path, sess_options, providers=providers)

        print(f"âœ… ONNXè¿è¡Œæ—¶åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - æ‰§è¡Œæä¾›è€…: {self.ort_session.get_providers()}")

        if scaler_path:
            self.load_scaler(scaler_path)

    def load_scaler(self, scaler_path):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        try:
            self.scaler = joblib.load(scaler_path)
            print(f"âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
        except Exception as e:
            print(f"âŒ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {str(e)}")
            self.scaler = None

    def load_test_data(self, data_file, sequence_length=10):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {data_file}")

        try:
            df = pd.read_excel(data_file, sheet_name='ä¸»æ•°æ®é›†')

            feature_columns = ['heart_rate', 'spo2', 'respiratory_rate', 'temperature']
            features = df[feature_columns].values
            labels = df['status_label'].values

            if self.scaler is not None:
                features = self.scaler.transform(features)
            else:
                print("âš ï¸  æœªåŠ è½½æ ‡å‡†åŒ–å™¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")

            sequences = []
            sequence_labels = []

            for i in range(len(features) - sequence_length + 1):
                seq = features[i:i + sequence_length]
                label = labels[i + sequence_length - 1]
                sequences.append(seq)
                sequence_labels.append(label)

            sequences = np.array(sequences, dtype=np.float32)
            sequence_labels = np.array(sequence_labels)

            print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"   - åºåˆ—æ•°é‡: {len(sequences)}")
            print(f"   - åºåˆ—å½¢çŠ¶: {sequences.shape}")
            print(f"   - æ­£å¸¸æ ·æœ¬: {np.sum(sequence_labels == 0)}")
            print(f"   - å¼‚å¸¸æ ·æœ¬: {np.sum(sequence_labels == 1)}")

            return sequences, sequence_labels

        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None, None

    def predict_batch(self, sequences):
        """æ‰¹é‡é¢„æµ‹"""
        try:
            input_name = self.ort_session.get_inputs()[0].name
            outputs = self.ort_session.run(None, {input_name: sequences})

            logits = outputs[0]
            probabilities = self._softmax(logits)
            predictions = np.argmax(logits, axis=1)

            return predictions, probabilities

        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}")
            return None, None

    def _softmax(self, x):
        """Softmaxå‡½æ•°"""
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def evaluate_model(self, test_sequences, test_labels, batch_size=32):
        """è¯„ä¼°ONNXæ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ“Š å¼€å§‹è¯„ä¼°ONNXæ¨¡å‹...")

        all_predictions = []
        all_probabilities = []
        inference_times = []

        num_batches = (len(test_sequences) + batch_size - 1) // batch_size

        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(test_sequences))
            batch_sequences = test_sequences[start_idx:end_idx]

            start_time = time.time()
            predictions, probabilities = self.predict_batch(batch_sequences)
            inference_time = time.time() - start_time

            if predictions is not None:
                all_predictions.extend(predictions)
                all_probabilities.extend(probabilities)
                inference_times.append(inference_time)

            if (i + 1) % 10 == 0:
                print(f"   å¤„ç†è¿›åº¦: {i + 1}/{num_batches} æ‰¹æ¬¡")

        if not all_predictions:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")
            return None

        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)

        accuracy = accuracy_score(test_labels, all_predictions)
        avg_inference_time = np.mean(inference_times)
        total_inference_time = np.sum(inference_times)

        print(f"\nğŸ“ˆ ONNXæ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"   - å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")
        print(f"   - æ€»æ ·æœ¬æ•°: {len(test_labels)}")
        print(f"   - å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time * 1000:.2f} ms/batch")
        print(f"   - æ€»æ¨ç†æ—¶é—´: {total_inference_time:.2f} s")
        print(f"   - å•æ ·æœ¬æ¨ç†æ—¶é—´: {total_inference_time / len(test_labels) * 1000:.2f} ms")

        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        class_names = ['æ­£å¸¸', 'å¼‚å¸¸']
        report = classification_report(test_labels, all_predictions,
                                       target_names=class_names, digits=4)
        print(report)

        self._plot_confusion_matrix(test_labels, all_predictions, class_names)

        return {
            'accuracy': accuracy,
            'predictions': all_predictions,
            'probabilities': all_probabilities,
            'inference_times': inference_times,
            'avg_inference_time': avg_inference_time
        }

    def _plot_confusion_matrix(self, y_true, y_pred, class_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.title('é«˜ç²¾åº¦ONNXæ¨¡å‹æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        plt.tight_layout()
        plt.savefig('advanced_onnx_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()


def main():
    """ä¸»å‡½æ•°ï¼šé«˜ç²¾åº¦æ¨¡å‹è½¬æ¢å’Œæµ‹è¯•æµç¨‹"""
    print("=" * 80)
    print("ğŸ”„ PyTorchæ¨¡å‹è½¬ONNXå·¥å…·ï¼ˆé«˜ç²¾åº¦ç‰ˆæœ¬ï¼‰")
    print("=" * 80)

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pytorch_model_path = 'best_health_model.pth'
    onnx_model_path = 'health_model_advanced.onnx'
    scaler_path = 'health_scaler.pkl'
    test_data_path = 'test_health_monitoring_dataset.xlsx'

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    import os
    required_files = [pytorch_model_path, scaler_path, test_data_path]
    missing_files = [f for f in required_files if not os.path.exists(f)]

    if missing_files:
        print(f"âŒ ç¼ºå°‘ä»¥ä¸‹æ–‡ä»¶:")
        for f in missing_files:
            print(f"   - {f}")
        return

    try:
        # 1. é«˜ç²¾åº¦è½¬æ¢æ¨¡å‹
        print("\n" + "=" * 50)
        print("ğŸ”„ é«˜ç²¾åº¦æ¨¡å‹è½¬æ¢é˜¶æ®µ")
        print("=" * 50)

        converter = AdvancedModelConverter()
        pytorch_model, model_config = converter.load_pytorch_model(pytorch_model_path)

        success = converter.convert_to_onnx(
            pytorch_model=pytorch_model,
            onnx_path=onnx_model_path,
            input_shape=(1, 10, 4),
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            opset_version=13
        )

        if not success:
            print("âŒ æ¨¡å‹è½¬æ¢å¤±è´¥")
            return

        # 2. æµ‹è¯•ONNXæ¨¡å‹
        print("\n" + "=" * 50)
        print("ğŸ§ª é«˜ç²¾åº¦ONNXæ¨¡å‹æµ‹è¯•é˜¶æ®µ")
        print("=" * 50)

        tester = ONNXModelTester(onnx_model_path, scaler_path)
        test_sequences, test_labels = tester.load_test_data(test_data_path)

        if test_sequences is not None:
            # æ¨¡å‹è¯„ä¼°
            results = tester.evaluate_model(test_sequences, test_labels)

            if results:
                print("\n" + "=" * 80)
                print("ğŸ‰ é«˜ç²¾åº¦æ¨¡å‹è½¬æ¢å’Œæµ‹è¯•å®Œæˆ!")
                print(f"âœ… ONNXæ¨¡å‹å·²ä¿å­˜: {onnx_model_path}")
                print(f"âœ… æ¨¡å‹å‡†ç¡®ç‡: {results['accuracy']:.4f}")

                if results['accuracy'] > 0.9:
                    print("ğŸŒŸ é«˜ç²¾åº¦è½¬æ¢æˆåŠŸï¼å‡†ç¡®ç‡ä¿æŒåœ¨90%ä»¥ä¸Š")
                elif results['accuracy'] > 0.8:
                    print("âœ… è½¬æ¢æˆåŠŸï¼å‡†ç¡®ç‡ä¿æŒåœ¨80%ä»¥ä¸Š")
                else:
                    print("âš ï¸  è½¬æ¢å®Œæˆï¼Œä½†å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™")

                print("=" * 80)

    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–åŒ…
    required_packages = [
        ('torch', 'torch'),
        ('onnx', 'onnx'),
        ('onnxruntime', 'onnxruntime'),
        ('pandas', 'pandas'),
        ('numpy', 'numpy'),
        ('scikit-learn', 'sklearn'),
        ('matplotlib', 'matplotlib'),
        ('seaborn', 'seaborn'),
        ('joblib', 'joblib')
    ]

    missing_packages = []

    for install_name, import_name in required_packages:
        try:
            __import__(import_name)
        except ImportError:
            missing_packages.append(install_name)

    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for pkg in missing_packages:
            print(f"   - {pkg}")
        print(f"\nè¯·è¿è¡Œ: pip install {' '.join(missing_packages)}")
    else:
        print("âœ… æ‰€æœ‰ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
        main()

