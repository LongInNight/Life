import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime
import joblib
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class HealthSequenceDataset(Dataset):
    """
    å¥åº·ç›‘æµ‹åºåˆ—æ•°æ®é›†
    ç”¨äºå¤„ç†æ—¶é—´åºåˆ—çš„ç”Ÿå‘½ä½“å¾æ•°æ®
    """

    def __init__(self, features, labels, sequence_length=10):
        """
        Args:
            features: ç‰¹å¾æ•°æ® (n_samples, n_features)
            labels: æ ‡ç­¾æ•°æ® (n_samples,)
            sequence_length: ç”¨äºé¢„æµ‹çš„å†å²åºåˆ—é•¿åº¦
        """
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
        self.sequence_length = sequence_length

        # åˆ›å»ºåºåˆ—
        self.sequences = []
        self.sequence_labels = []

        # ä»ç‰¹å¾æ•°æ®ä¸­åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—
        for i in range(len(features) - sequence_length + 1):
            seq = self.features[i:i + sequence_length]  # (sequence_length, n_features)
            label = self.labels[i + sequence_length - 1]  # é¢„æµ‹æœ€åä¸€ä¸ªæ—¶åˆ»çš„æ ‡ç­¾

            self.sequences.append(seq)
            self.sequence_labels.append(label)

        self.sequences = torch.stack(self.sequences)  # (n_sequences, sequence_length, n_features)
        self.sequence_labels = torch.stack(self.sequence_labels)  # (n_sequences,)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.sequence_labels[idx]


class PatientSequenceDataset(Dataset):
    """
    æŒ‰æ‚£è€…åˆ†ç»„çš„å¥åº·ç›‘æµ‹åºåˆ—æ•°æ®é›†
    ä¿æŒæ‚£è€…å†…éƒ¨çš„æ—¶é—´è¿ç»­æ€§
    """

    def __init__(self, features, labels, patient_groups, sequence_length=10):
        """
        Args:
            features: ç‰¹å¾æ•°æ® (n_samples, n_features)
            labels: æ ‡ç­¾æ•°æ® (n_samples,)
            patient_groups: æ‚£è€…åˆ†ç»„ä¿¡æ¯ (n_samples,)
            sequence_length: åºåˆ—é•¿åº¦
        """
        self.sequence_length = sequence_length
        self.sequences = []
        self.sequence_labels = []
        self.sequence_info = []

        # æŒ‰æ‚£è€…åˆ†ç»„å¤„ç†
        unique_patients = np.unique(patient_groups)

        for patient_id in unique_patients:
            # è·å–è¯¥æ‚£è€…çš„æ‰€æœ‰æ•°æ®
            patient_mask = patient_groups == patient_id
            patient_features = features[patient_mask]
            patient_labels = labels[patient_mask]

            # ä¸ºè¯¥æ‚£è€…åˆ›å»ºåºåˆ—ï¼ˆä¿æŒæ—¶é—´é¡ºåºï¼‰
            if len(patient_features) >= sequence_length:
                for i in range(len(patient_features) - sequence_length + 1):
                    seq = patient_features[i:i + sequence_length]  # (sequence_length, n_features)
                    label = patient_labels[i + sequence_length - 1]  # é¢„æµ‹æœ€åæ—¶åˆ»çš„æ ‡ç­¾

                    self.sequences.append(torch.FloatTensor(seq))
                    self.sequence_labels.append(torch.LongTensor([label]))
                    self.sequence_info.append({
                        'patient_id': patient_id,
                        'start_idx': i,
                        'end_idx': i + sequence_length - 1
                    })

        print(f"   âœ… åˆ›å»ºäº† {len(self.sequences)} ä¸ªåºåˆ—ï¼Œæ¥è‡ª {len(unique_patients)} ä¸ªæ‚£è€…")

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.sequence_labels[idx].squeeze()


class LSTMHealthMonitor(nn.Module):
    """
    åŸºäºLSTMçš„å¥åº·ç›‘æµ‹æ¨¡å‹
    """

    def __init__(self, input_size=4, hidden_size=64, num_layers=2, num_classes=2, dropout=0.2):
        super(LSTMHealthMonitor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )

    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        batch_size = x.size(0)

        # LSTMå‰å‘ä¼ æ’­
        lstm_out, _ = self.lstm(x)  # (batch_size, sequence_length, hidden_size * 2)

        # æ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        final_out = attn_out[:, -1, :]  # (batch_size, hidden_size * 2)

        # åˆ†ç±»
        output = self.classifier(final_out)  # (batch_size, num_classes)

        return output


class HealthDataLoader:
    """
    å¥åº·æ•°æ®åŠ è½½å™¨
    ä¸“é—¨ç”¨äºåŠ è½½å’Œé¢„å¤„ç†xlsxæ ¼å¼çš„å¥åº·ç›‘æµ‹æ•°æ®
    """

    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_columns = ['heart_rate', 'spo2', 'respiratory_rate', 'temperature']
        self.is_fitted = False

    def load_xlsx_data(self, file_path, sheet_name='ä¸»æ•°æ®é›†'):
        """
        ä»xlsxæ–‡ä»¶åŠ è½½æ•°æ®
        """
        try:
            print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")

            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path, sheet_name=sheet_name)

            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
            print(f"   - æ•°æ®å½¢çŠ¶: {df.shape}")
            print(f"   - åˆ—å: {list(df.columns)}")

            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = self.feature_columns + ['status_label']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")

            # æ•°æ®åŸºæœ¬ç»Ÿè®¡
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   - æ€»æ ·æœ¬æ•°: {len(df):,}")
            print(f"   - æ­£å¸¸æ ·æœ¬: {len(df[df['status_label'] == 0]):,}")
            print(f"   - å¼‚å¸¸æ ·æœ¬: {len(df[df['status_label'] == 1]):,}")
            print(f"   - å¼‚å¸¸ç‡: {len(df[df['status_label'] == 1]) / len(df):.2%}")

            return df

        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise

    def preprocess_data(self, df, fit_scaler=True):
        """
        é¢„å¤„ç†æ•°æ®
        """
        print("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        print(f"   - æ£€æŸ¥ç¼ºå¤±å€¼...")
        missing_info = df[self.feature_columns + ['status_label']].isnull().sum()
        if missing_info.sum() > 0:
            print(f"   âš ï¸  å‘ç°ç¼ºå¤±å€¼:\n{missing_info}")
            # å¡«å……ç¼ºå¤±å€¼
            df = df.fillna(method='ffill').fillna(method='bfill')
            print(f"   âœ… ç¼ºå¤±å€¼å·²å¡«å……")

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        features = df[self.feature_columns].values
        labels = df['status_label'].values

        # æ ‡å‡†åŒ–ç‰¹å¾
        if fit_scaler:
            print(f"   - æ‹Ÿåˆæ ‡å‡†åŒ–å™¨...")
            features_scaled = self.scaler.fit_transform(features)
            self.is_fitted = True
        else:
            if not self.is_fitted:
                raise ValueError("æ ‡å‡†åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆåœ¨è®­ç»ƒæ•°æ®ä¸Šè°ƒç”¨fit_scaler=True")
            print(f"   - åº”ç”¨æ ‡å‡†åŒ–å™¨...")
            features_scaled = self.scaler.transform(features)

        # æå–æ‚£è€…åˆ†ç»„ä¿¡æ¯
        if 'sequence_id' in df.columns:
            patient_info = df['sequence_id'].str.extract(r'(P\d+)_(\d+)')
            patient_groups = patient_info[0].values  # æ‚£è€…ID
        else:
            patient_groups = None

        print(f"   âœ… é¢„å¤„ç†å®Œæˆ!")
        print(f"   - ç‰¹å¾å½¢çŠ¶: {features_scaled.shape}")
        print(f"   - æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"   - ç‰¹å¾èŒƒå›´: [{features_scaled.min():.3f}, {features_scaled.max():.3f}]")

        return features_scaled, labels, patient_groups

    def save_scaler(self, file_path):
        """ä¿å­˜æ ‡å‡†åŒ–å™¨"""
        if self.is_fitted:
            joblib.dump(self.scaler, file_path)
            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {file_path}")
        else:
            print("âš ï¸  æ ‡å‡†åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œæ— æ³•ä¿å­˜")

    def load_scaler(self, file_path):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        try:
            self.scaler = joblib.load(file_path)
            self.is_fitted = True
            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {file_path}")
        except Exception as e:
            print(f"âŒ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {str(e)}")
            raise


class HealthModelTrainer:
    """
    å¥åº·ç›‘æµ‹æ¨¡å‹è®­ç»ƒå™¨
    """

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }

    def train_model(self, train_loader, val_loader=None, epochs=100, lr=0.001,
                    weight_decay=1e-4, patience=15, save_path='best_model.pth'):
        """
        è®­ç»ƒæ¨¡å‹
        """
        print(f"ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        if val_loader:
            print(f"   - éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {train_loader.batch_size}")
        print(f"   - å­¦ä¹ ç‡: {lr}")
        print(f"   - æœ€å¤§è½®æ•°: {epochs}")

        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
        criterion = nn.CrossEntropyLoss()

        # æ—©åœæœºåˆ¶
        best_val_loss = float('inf')
        patience_counter = 0

        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒ...")
        print("=" * 80)

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0

            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)

                # æ£€æŸ¥æ•°æ®ç»´åº¦
                if len(data.shape) != 3:
                    print(f"âŒ æ•°æ®ç»´åº¦é”™è¯¯: {data.shape}, æœŸæœ›: (batch_size, sequence_length, input_size)")
                    raise ValueError(f"æ•°æ®ç»´åº¦é”™è¯¯: {data.shape}")

                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()

                train_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                train_total += target.size(0)
                train_correct += (predicted == target).sum().item()

            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = train_loss / len(train_loader)
            train_accuracy = 100. * train_correct / train_total

            self.training_history['train_loss'].append(avg_train_loss)
            self.training_history['train_acc'].append(train_accuracy)

            # éªŒè¯é˜¶æ®µ
            if val_loader:
                val_loss, val_accuracy = self._validate(val_loader, criterion)
                self.training_history['val_loss'].append(val_loss)
                self.training_history['val_acc'].append(val_accuracy)

                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_loss)

                # æ—©åœæ£€æŸ¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'train_loss': avg_train_loss,
                        'val_loss': val_loss,
                        'train_acc': train_accuracy,
                        'val_acc': val_accuracy
                    }, save_path)
                else:
                    patience_counter += 1

                # æ‰“å°è¿›åº¦
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    print(f"Epoch [{epoch + 1:3d}/{epochs}] | "
                          f"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | "
                          f"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | "
                          f"LR: {optimizer.param_groups[0]['lr']:.6f}")

                # æ—©åœ
                if patience_counter >= patience:
                    print(f"\nâ° æ—©åœè§¦å‘! éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„")
                    break
            else:
                # æ²¡æœ‰éªŒè¯é›†æ—¶çš„å¤„ç†
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    print(f"Epoch [{epoch + 1:3d}/{epochs}] | "
                          f"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%")

                # ä¿å­˜æ¨¡å‹ï¼ˆæ²¡æœ‰éªŒè¯é›†æ—¶ï¼‰
                if epoch == 0 or avg_train_loss < best_val_loss:
                    best_val_loss = avg_train_loss
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'train_loss': avg_train_loss,
                        'train_acc': train_accuracy
                    }, save_path)

        print("\n" + "=" * 80)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³æŸå¤±: {best_val_loss:.4f}")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
        print("=" * 80)

    def _validate(self, val_loader, criterion):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                val_loss += criterion(output, target).item()

                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = 100. * val_correct / val_total

        return avg_val_loss, val_accuracy

    def evaluate_model(self, test_loader, model_path=None):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        """
        if model_path:
            # åŠ è½½æœ€ä½³æ¨¡å‹
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print(f"ğŸ“‚ å·²åŠ è½½æ¨¡å‹: {model_path}")

        self.model.eval()
        all_predictions = []
        all_targets = []
        all_probabilities = []

        print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)

                # è·å–æ¦‚ç‡
                probabilities = torch.softmax(output, dim=1)

                _, predicted = torch.max(output, 1)

                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_targets, all_predictions)

        print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"   - å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        class_names = ['æ­£å¸¸', 'å¼‚å¸¸']
        report = classification_report(all_targets, all_predictions,
                                       target_names=class_names, digits=4)
        print(report)

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_targets, all_predictions)
        self._plot_confusion_matrix(cm, class_names)

        return {
            'accuracy': accuracy,
            'predictions': all_predictions,
            'targets': all_targets,
            'probabilities': all_probabilities,
            'confusion_matrix': cm
        }

    def _plot_confusion_matrix(self, cm, class_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.title('æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        plt.tight_layout()
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if not self.training_history['train_loss']:
            print("âš ï¸  æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # æŸå¤±æ›²çº¿
        epochs = range(1, len(self.training_history['train_loss']) + 1)
        ax1.plot(epochs, self.training_history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        if self.training_history['val_loss']:
            ax1.plot(epochs, self.training_history['val_loss'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_title('æ¨¡å‹æŸå¤±', fontsize=14, fontweight='bold')
        ax1.set_xlabel('è½®æ•°', fontsize=12)
        ax1.set_ylabel('æŸå¤±', fontsize=12)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, self.training_history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        if self.training_history['val_acc']:
            ax2.plot(epochs, self.training_history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax2.set_title('æ¨¡å‹å‡†ç¡®ç‡', fontsize=14, fontweight='bold')
        ax2.set_xlabel('è½®æ•°', fontsize=12)
        ax2.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()


def main():
    """
    ä¸»è®­ç»ƒæµç¨‹
    """
    print("=" * 80)
    print("ğŸ¥ å¥åº·ç›‘æµ‹æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    print("\n" + "=" * 50)
    print("ğŸ“‚ æ•°æ®åŠ è½½é˜¶æ®µ")
    print("=" * 50)

    data_loader = HealthDataLoader()

    # åŠ è½½è®­ç»ƒæ•°æ®
    train_df = data_loader.load_xlsx_data('train_health_monitoring_dataset.xlsx')
    train_features, train_labels, train_patients = data_loader.preprocess_data(
        train_df, fit_scaler=True
    )

    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    data_loader.save_scaler('health_scaler3.pkl')

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_df = data_loader.load_xlsx_data('test_health_monitoring_dataset.xlsx')
    test_features, test_labels, test_patients = data_loader.preprocess_data(
        test_df, fit_scaler=False
    )

    # 2. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("\n" + "=" * 50)
    print("ğŸ”§ æ•°æ®é›†å‡†å¤‡é˜¶æ®µ")
    print("=" * 50)

    sequence_length = 10

    # åˆ›å»ºæ•°æ®é›†
    if train_patients is not None:
        print("ğŸ‘¥ ä½¿ç”¨æŒ‰æ‚£è€…åˆ†ç»„çš„æ•°æ®é›†...")
        train_dataset = PatientSequenceDataset(train_features, train_labels, train_patients, sequence_length)
        test_dataset = PatientSequenceDataset(test_features, test_labels, test_patients, sequence_length)
    else:
        print("ğŸ“Š ä½¿ç”¨æ™®é€šåºåˆ—æ•°æ®é›†...")
        train_dataset = HealthSequenceDataset(train_features, train_labels, sequence_length)
        test_dataset = HealthSequenceDataset(test_features, test_labels, sequence_length)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸æ‰“ä¹±æ•°æ®ï¼‰
    batch_size = 32
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=False,  # ä¸æ‰“ä¹±æ•°æ®
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,  # ä¸æ‰“ä¹±æ•°æ®
        num_workers=0,
        pin_memory=True if device.type == 'cuda' else False
    )

    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
    print(f"   - è®­ç»ƒåºåˆ—: {len(train_dataset)}")
    print(f"   - æµ‹è¯•åºåˆ—: {len(test_dataset)}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"   - ç‰¹å¾ç»´åº¦: 4")

    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    sample_data, sample_label = train_dataset[0]
    print(f"   - æ ·æœ¬æ•°æ®å½¢çŠ¶: {sample_data.shape}")
    print(f"   - æ ·æœ¬æ ‡ç­¾å½¢çŠ¶: {sample_label.shape if hasattr(sample_label, 'shape') else type(sample_label)}")

    # 3. æ¨¡å‹åˆ›å»ºå’Œè®­ç»ƒ
    print("\n" + "=" * 50)
    print("ğŸ¤– æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
    print("=" * 50)

    # åˆ›å»ºæ¨¡å‹
    model = LSTMHealthMonitor(
        input_size=4,  # 4ä¸ªç”Ÿå‘½ä½“å¾ç‰¹å¾
        hidden_size=64,  # LSTMéšè—å±‚å¤§å°
        num_layers=2,  # LSTMå±‚æ•°
        num_classes=2,  # äºŒåˆ†ç±»
        dropout=0.2  # Dropoutç‡
    )

    print(f"ğŸ—ï¸  æ¨¡å‹æ¶æ„:")
    print(f"   - è¾“å…¥ç‰¹å¾: 4 (å¿ƒç‡ã€è¡€æ°§ã€å‘¼å¸é¢‘ç‡ã€ä½“æ¸©)")
    print(f"   - LSTMéšè—å±‚: {model.hidden_size}")
    print(f"   - LSTMå±‚æ•°: {model.num_layers}")
    print(f"   - è¾“å‡ºç±»åˆ«: 2 (æ­£å¸¸/å¼‚å¸¸)")
    print(f"   - æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HealthModelTrainer(model, device)

    # å¼€å§‹è®­ç»ƒ
    trainer.train_model(
        train_loader=train_loader,
        val_loader=None,  # å¯ä»¥è®¾ç½®éªŒè¯é›†
        epochs=100,
        lr=0.001,
        weight_decay=1e-4,
        patience=15,
        save_path='best_health_model3.pth'
    )

    # 4. æ¨¡å‹è¯„ä¼°
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°é˜¶æ®µ")
    print("=" * 50)

    # è¯„ä¼°æ¨¡å‹
    results = trainer.evaluate_model(test_loader, 'best_health_model3.pth')

    # ç»˜åˆ¶è®­ç»ƒå†å²
    trainer.plot_training_history()

    # 5. ä¿å­˜è®­ç»ƒä¿¡æ¯
    print("\n" + "=" * 50)
    print("ğŸ’¾ ä¿å­˜è®­ç»ƒä¿¡æ¯")
    print("=" * 50)

    # ä¿å­˜è®­ç»ƒé…ç½®å’Œç»“æœ
    training_info = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'model_config': {
            'input_size': 4,
            'hidden_size': 64,
            'num_layers': 2,
            'num_classes': 2,
            'dropout': 0.2
        },
        'training_config': {
            'epochs': 100,
            'batch_size': batch_size,
            'learning_rate': 0.001,
            'weight_decay': 1e-4,
            'sequence_length': sequence_length
        },
        'data_info': {
            'train_samples': len(train_dataset),
            'test_samples': len(test_dataset),
            'train_file': 'train_health_monitoring_dataset.xlsx',
            'test_file': 'test_health_monitoring_dataset.xlsx',
            'features': ['heart_rate', 'spo2', 'respiratory_rate', 'temperature']
        },
        'results': {
            'test_accuracy': results['accuracy'],
            'confusion_matrix': results['confusion_matrix'].tolist()
        }
    }
    # ä¿å­˜è®­ç»ƒä¿¡æ¯åˆ°JSONæ–‡ä»¶
    import json
    with open('training_info3.json', 'w', encoding='utf-8') as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)

    print(f"âœ… è®­ç»ƒä¿¡æ¯å·²ä¿å­˜è‡³: training_info3.json")
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: best_health_model3.pth")
    print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: health_scaler3.pkl")

    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
    print("=" * 80)

    return model, trainer, results


def load_and_predict(model_path, scaler_path, data_file, sequence_length=10):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    """
    print("ğŸ”® åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹...")

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ•°æ®å¤„ç†å™¨
    data_loader = HealthDataLoader()
    data_loader.load_scaler(scaler_path)

    # åŠ è½½æ•°æ®
    df = data_loader.load_xlsx_data(data_file)
    features, labels, patients = data_loader.preprocess_data(df, fit_scaler=False)

    # åˆ›å»ºæ•°æ®é›†
    if patients is not None:
        dataset = PatientSequenceDataset(features, labels, patients, sequence_length)
        seq_info = dataset.sequence_info
    else:
        dataset = HealthSequenceDataset(features, labels, sequence_length)
        seq_info = None

    data_loader_pred = DataLoader(dataset, batch_size=32, shuffle=False)

    # åŠ è½½æ¨¡å‹
    model = LSTMHealthMonitor(input_size=4, hidden_size=64, num_layers=2, num_classes=2)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    # è¿›è¡Œé¢„æµ‹
    predictions = []
    probabilities = []

    with torch.no_grad():
        for data, _ in data_loader_pred:
            data = data.to(device)
            output = model(data)
            probs = torch.softmax(output, dim=1)
            _, preds = torch.max(output, 1)

            predictions.extend(preds.cpu().numpy())
            probabilities.extend(probs.cpu().numpy())

    return predictions, probabilities, seq_info


def analyze_patient_predictions(predictions, probabilities, seq_info, output_file='prediction_analysis.xlsx'):
    """
    åˆ†ææ‚£è€…çº§åˆ«çš„é¢„æµ‹ç»“æœ
    """
    print("ğŸ“Š åˆ†ææ‚£è€…é¢„æµ‹ç»“æœ...")

    if seq_info is None:
        print("âš ï¸  æ²¡æœ‰æ‚£è€…åºåˆ—ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ‚£è€…çº§åˆ«åˆ†æ")
        return None

    # æŒ‰æ‚£è€…æ±‡æ€»é¢„æµ‹ç»“æœ
    patient_results = {}

    for i, info in enumerate(seq_info):
        patient_id = info['patient_id']
        pred = predictions[i]
        prob = probabilities[i]

        if patient_id not in patient_results:
            patient_results[patient_id] = {
                'predictions': [],
                'probabilities': [],
                'sequence_count': 0
            }

        patient_results[patient_id]['predictions'].append(pred)
        patient_results[patient_id]['probabilities'].append(prob)
        patient_results[patient_id]['sequence_count'] += 1

    # ç”Ÿæˆæ‚£è€…çº§åˆ«çš„ç»Ÿè®¡
    patient_summary = []

    for patient_id, data in patient_results.items():
        preds = np.array(data['predictions'])
        probs = np.array(data['probabilities'])

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_sequences = len(preds)
        anomaly_sequences = np.sum(preds == 1)
        normal_sequences = np.sum(preds == 0)
        anomaly_rate = anomaly_sequences / total_sequences

        # å¹³å‡æ¦‚ç‡
        avg_normal_prob = np.mean(probs[:, 0])
        avg_anomaly_prob = np.mean(probs[:, 1])

        # æœ€å¤§å¼‚å¸¸æ¦‚ç‡
        max_anomaly_prob = np.max(probs[:, 1])

        # æ‚£è€…æ•´ä½“çŠ¶æ€åˆ¤æ–­
        if anomaly_rate > 0.3:  # å¦‚æœè¶…è¿‡30%çš„åºåˆ—è¢«é¢„æµ‹ä¸ºå¼‚å¸¸
            patient_status = "é«˜é£é™©"
        elif anomaly_rate > 0.1:
            patient_status = "ä¸­é£é™©"
        else:
            patient_status = "ä½é£é™©"

        patient_summary.append({
            'patient_id': patient_id,
            'total_sequences': total_sequences,
            'normal_sequences': normal_sequences,
            'anomaly_sequences': anomaly_sequences,
            'anomaly_rate': f"{anomaly_rate:.2%}",
            'avg_normal_prob': f"{avg_normal_prob:.3f}",
            'avg_anomaly_prob': f"{avg_anomaly_prob:.3f}",
            'max_anomaly_prob': f"{max_anomaly_prob:.3f}",
            'patient_status': patient_status
        })

    # ä¿å­˜åˆ†æç»“æœ
    df_summary = pd.DataFrame(patient_summary)
    df_summary.to_excel(output_file, index=False)

    print(f"âœ… æ‚£è€…é¢„æµ‹åˆ†æå·²ä¿å­˜è‡³: {output_file}")
    print(f"\nğŸ“‹ æ‚£è€…é£é™©åˆ†å¸ƒ:")
    status_counts = df_summary['patient_status'].value_counts()
    for status, count in status_counts.items():
        print(f"   - {status}: {count} äºº")

    return df_summary


def visualize_patient_timeline(patient_id, data_file, model_path, scaler_path,
                               sequence_length=10, output_file=None):
    """
    å¯è§†åŒ–ç‰¹å®šæ‚£è€…çš„æ—¶é—´çº¿é¢„æµ‹ç»“æœ
    """
    print(f"ğŸ“ˆ ç”Ÿæˆæ‚£è€… {patient_id} çš„æ—¶é—´çº¿å¯è§†åŒ–...")

    try:
        # åŠ è½½æ•°æ®å’Œæ¨¡å‹
        predictions, probabilities, seq_info = load_and_predict(
            model_path, scaler_path, data_file, sequence_length
        )

        # åŠ è½½åŸå§‹æ•°æ®
        data_loader = HealthDataLoader()
        df = data_loader.load_xlsx_data(data_file)

        # ç­›é€‰ç‰¹å®šæ‚£è€…çš„æ•°æ®
        if 'sequence_id' in df.columns:
            patient_data = df[df['sequence_id'].str.startswith(patient_id)]
        else:
            print(f"âŒ æ•°æ®ä¸­æ²¡æœ‰sequence_idåˆ—ï¼Œæ— æ³•ç­›é€‰æ‚£è€…æ•°æ®")
            return

        if len(patient_data) == 0:
            print(f"âŒ æœªæ‰¾åˆ°æ‚£è€… {patient_id} çš„æ•°æ®")
            return

        # ç­›é€‰è¯¥æ‚£è€…çš„é¢„æµ‹ç»“æœ
        patient_predictions = []
        patient_probabilities = []

        if seq_info:
            for i, info in enumerate(seq_info):
                if info['patient_id'] == patient_id:
                    patient_predictions.append(predictions[i])
                    patient_probabilities.append(probabilities[i])

        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(3, 2, figsize=(20, 15))
        fig.suptitle(f'æ‚£è€… {patient_id} å¥åº·ç›‘æµ‹æ—¶é—´çº¿åˆ†æ', fontsize=16, fontweight='bold')

        # æ—¶é—´è½´
        time_points = range(len(patient_data))

        # 1. ç”Ÿå‘½ä½“å¾æ•°æ®
        features = ['heart_rate', 'spo2', 'respiratory_rate', 'temperature']
        feature_names = ['å¿ƒç‡ (bpm)', 'è¡€æ°§é¥±å’Œåº¦ (%)', 'å‘¼å¸é¢‘ç‡ (æ¬¡/åˆ†)', 'ä½“æ¸© (Â°C)']

        for i, (feature, name) in enumerate(zip(features, feature_names)):
            ax = axes[i // 2, i % 2]

            # ç»˜åˆ¶åŸå§‹æ•°æ®
            values = patient_data[feature].values
            ax.plot(time_points, values, 'b-', linewidth=2, alpha=0.7, label='å®é™…å€¼')

            # æ ‡è®°å¼‚å¸¸ç‚¹
            anomaly_mask = patient_data['status_label'].values == 1
            ax.scatter(np.array(time_points)[anomaly_mask], values[anomaly_mask],
                       c='red', s=30, alpha=0.8, label='å®é™…å¼‚å¸¸', zorder=5)

            # æ·»åŠ æ­£å¸¸èŒƒå›´
            normal_ranges = {
                'heart_rate': (60, 100),
                'spo2': (95, 100),
                'respiratory_rate': (12, 20),
                'temperature': (36.1, 37.2)
            }

            if feature in normal_ranges:
                min_val, max_val = normal_ranges[feature]
                ax.axhline(y=min_val, color='green', linestyle='--', alpha=0.5)
                ax.axhline(y=max_val, color='green', linestyle='--', alpha=0.5)
                ax.fill_between(time_points, min_val, max_val, alpha=0.1, color='green', label='æ­£å¸¸èŒƒå›´')

            ax.set_title(name, fontsize=12, fontweight='bold')
            ax.set_xlabel('æ—¶é—´ (åˆ†é’Ÿ)')
            ax.set_ylabel(name)
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 2. é¢„æµ‹æ¦‚ç‡æ—¶é—´çº¿
        if patient_predictions:
            ax = axes[2, 0]
            pred_time_points = range(sequence_length - 1, sequence_length - 1 + len(patient_predictions))
            anomaly_probs = [prob[1] for prob in patient_probabilities]

            ax.plot(pred_time_points, anomaly_probs, 'r-', linewidth=2, label='å¼‚å¸¸æ¦‚ç‡')
            ax.fill_between(pred_time_points, 0, anomaly_probs, alpha=0.3, color='red')
            ax.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='å†³ç­–é˜ˆå€¼')

            ax.set_title('æ¨¡å‹é¢„æµ‹å¼‚å¸¸æ¦‚ç‡', fontsize=12, fontweight='bold')
            ax.set_xlabel('æ—¶é—´ (åˆ†é’Ÿ)')
            ax.set_ylabel('å¼‚å¸¸æ¦‚ç‡')
            ax.set_ylim(0, 1)
            ax.legend()
            ax.grid(True, alpha=0.3)

        # 3. é¢„æµ‹ç»“æœå¯¹æ¯”
        ax = axes[2, 1]

        # å®é™…æ ‡ç­¾
        actual_labels = patient_data['status_label'].values
        ax.plot(time_points, actual_labels + 0.1, 'g-', linewidth=3, alpha=0.7, label='å®é™…æ ‡ç­¾')

        # é¢„æµ‹æ ‡ç­¾
        if patient_predictions:
            pred_labels = np.array(patient_predictions)
            pred_time_points = range(sequence_length - 1, sequence_length - 1 + len(pred_labels))
            ax.plot(pred_time_points, pred_labels - 0.1, 'r-', linewidth=3, alpha=0.7, label='é¢„æµ‹æ ‡ç­¾')

        ax.set_title('é¢„æµ‹ç»“æœå¯¹æ¯”', fontsize=12, fontweight='bold')
        ax.set_xlabel('æ—¶é—´ (åˆ†é’Ÿ)')
        ax.set_ylabel('çŠ¶æ€ (0=æ­£å¸¸, 1=å¼‚å¸¸)')
        ax.set_ylim(-0.5, 1.5)
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()

        if output_file:
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜è‡³: {output_file}")

        plt.show()

    except Exception as e:
        print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–åŒ…
    required_packages = [
        ('torch', 'torch'),
        ('pandas', 'pandas'),
        ('numpy', 'numpy'),
        ('scikit-learn', 'sklearn'),
        ('matplotlib', 'matplotlib'),
        ('seaborn', 'seaborn'),
        ('openpyxl', 'openpyxl'),
        ('joblib', 'joblib')
    ]

    missing_packages = []

    for install_name, import_name in required_packages:
        try:
            __import__(import_name)
        except ImportError:
            missing_packages.append(install_name)

    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for pkg in missing_packages:
            print(f"   - {pkg}")
        print("\nè¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
    else:
        print("âœ… æ‰€æœ‰ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")

        # è¿è¡Œä¸»è®­ç»ƒæµç¨‹
        try:
            model, trainer, results = main()

            # å¯é€‰: è¿›è¡Œé¢„æµ‹åˆ†æ
            print("\n" + "=" * 50)
            print("ğŸ”® é¢„æµ‹åˆ†æé˜¶æ®µ")
            print("=" * 50)

            try:
                # å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹åˆ†æ
                predictions, probabilities, seq_info = load_and_predict(
                    'best_health_model3.pth',
                    'health_scaler3.pkl',
                    'test_health_monitoring_dataset.xlsx'
                )

                # ç”Ÿæˆæ‚£è€…çº§åˆ«åˆ†æ
                patient_analysis = analyze_patient_predictions(
                    predictions, probabilities, seq_info
                )

                # å¯è§†åŒ–ç¤ºä¾‹æ‚£è€…
                if seq_info and len(seq_info) > 0:
                    sample_patient = seq_info[0]['patient_id']
                    visualize_patient_timeline(
                        sample_patient,
                        'test_health_monitoring_dataset.xlsx',
                        'best_health_model3.pth',
                        'health_scaler3.pkl',
                        output_file=f'{sample_patient}_timeline3.png'
                    )

            except Exception as e:
                print(f"âš ï¸  é¢„æµ‹åˆ†æå‡ºç°é”™è¯¯: {str(e)}")
                print("ä¸»è®­ç»ƒæµç¨‹å·²å®Œæˆï¼Œå¯ä»¥æ‰‹åŠ¨è¿è¡Œé¢„æµ‹åˆ†æ")

        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}")
            import traceback

            traceback.print_exc()

